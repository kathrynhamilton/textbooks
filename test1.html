<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
  <meta http-equiv="Content-Style-Type" content="text/css" />
  <meta name="generator" content="pandoc" />
  <title></title>
  <style type="text/css">code{white-space: pre;}</style>
</head>
<body>
<h1 id="an-introduction-to-statistical-learning-with-applications-in-r">An Introduction to Statistical Learning with Applications in R</h1>
<p>James, Witten, Hastie, Tibshirani</p>
<p>Kathryn Hamilton - March 2018</p>
<h2 id="introduction">1 - Introduction</h2>
<p>Statistical learning is a set of tools for modeling and understanding complex datasets. These tools can be classified as <strong>supervised</strong> or <strong>unsupervised</strong>.</p>
<p>Supervised learning predicts an output based on one or more inputs. Unsupervised learning has inputs and no supervised output, but can still be used to learn relationships and structure.</p>
<p>Three example datasets are used throughout this book:</p>
<ul>
<li><code>Wage</code> - Income information for males in the central Atlantic region of the United States from 2003 to 2009. The goal is to predict income based on education, age, and year (Regression).</li>
<li><code>Smarket</code> - Daily return information of the S&amp;P stock index from 2001 to 2005. The goal is to predict whether the market will increase or decrease on a given day (Classification).</li>
<li><code>NCI60</code> - Gene expression data for 64 cancer cell lines. The goal is to determine whether there are groups or clusters among the cell lines based on their expression measurements (Clustering).</li>
</ul>
<p>There are <strong>R</strong> libraries and the above datasets available on the book's <a href="http:/www.StatLearning.com">website</a> to work through the book's examples and labs.</p>
<h2 id="statistical-learning">2 - Statistical Learning</h2>
<h3 id="what-is-statistical-learning">What is Statistical Learning?</h3>
<p>For input variables <span class="math inline"><em>X</em><sub>1</sub>, <em>X</em><sub>2</sub>, …, <em>X</em><sub><em>p</em></sub></span> and output response <span class="math inline"><em>Y</em></span> we assume that there is some relationship <span class="math inline"><em>Y</em> = <em>f</em>(<em>X</em>)+<em>ϵ</em></span>, where <span class="math inline"><em>ϵ</em></span> is a zero-mean random error term independent of <span class="math inline"><em>X</em></span>. Statistical learning is a set of approaches to estimate <span class="math inline"><em>f</em></span>.</p>
<p>There are two reasons to estimate <span class="math inline"><em>f</em></span>: prediction and inference.</p>
<p>In situations where <span class="math inline"><em>X</em></span> is readily available but <span class="math inline"><em>Y</em></span> is not, we can <strong>predict</strong> <span class="math inline"><em>Y</em></span> using an estimate of <span class="math inline"><em>f</em></span>: <span class="math inline">$\hat{Y} = \hat(f)(X)$</span>. In general, <span class="math inline">$\hat{f}$</span> is not a perfect estimate for <span class="math inline"><em>f</em></span> because it cannot predict the irreducible error <span class="math inline"><em>ϵ</em></span>. Here we can treat <span class="math inline">$\hat{f}$</span> as a black box.</p>
<p>In other situations, we are interested in understanding the way that <span class="math inline"><em>Y</em></span> is affected as <span class="math inline"><em>X</em></span> changes. This is called <strong>inference</strong>. We still need to estimate <span class="math inline"><em>f</em></span> but cannot treat <span class="math inline">$\hat{f}$</span> as a black box as we are interested in understanding the relationship between <span class="math inline"><em>X</em></span> and <span class="math inline"><em>Y</em></span> as opposed to predicting <span class="math inline"><em>Y</em></span>. We may want to know:</p>
<ul>
<li>Which predictors are most strongly associated to the response?</li>
<li>What is the relationship between the response and each predictors?</li>
<li>Which relationships can be summarized using a linear equation? Which are more complicated?</li>
</ul>
<p>Many problems fall into both of these categories. For example, consider an advertising firm that would like to know which form of media advertising generates the largest boost in sales (inference), and how much of an increase in sales would occur given an advertising increase in that specific media (prediction).</p>
<p>The best method for estimating <span class="math inline"><em>f</em></span> depends on the goal and what you are trying to achieve.</p>
<p>Most statistical learning methods can be classified as either <strong>parametric</strong> or <strong>non-parametric</strong>.</p>
<p>Parametric methods first make an assumption about the form of <span class="math inline"><em>f</em></span>, which typical involves some parameters, and then uses training data to fit the model and thus provide estimate values for each parameter. Assuming a parametric form simplifies the problem of estimating <span class="math inline"><em>f</em></span>, but can be dangerous if the model chosen does not represent the data sufficiently well.</p>
<p>Non-parametric methods do not make an explicit assumption about the form of <span class="math inline"><em>f</em></span>. They seek an estimate of <span class="math inline"><em>f</em></span> by getting as close to the data as possible while limiting noise and overfitting. The advantage is that they can be fit accuractely to a wide range of possible shapes, thus avoiding the possibility of poor fit. However, solving these types of problems are can be very difficult and the results are often not easy to interpret.</p>
<p>Parsimony, the art of expressing a function as simply as possible yet with sufficient detail, is an important topic in both methods, and is necessary to prevent overfitting.</p>
<h3 id="assessing-model-accuracy">Assessing Model Accuracy</h3>
<h2 id="linear-regression">3 - Linear Regression</h2>
<h3 id="simple-linear-regression">Simple Linear Regression</h3>
<h3 id="multiple-linear-regression">Multiple Linear Regression</h3>
<h3 id="other-consideration-in-the-regression-model">Other Consideration in the Regression Model</h3>
<h3 id="the-marketing-plan">The Marketing Plan</h3>
<h3 id="comparison-of-linear-regression-with-k-nearest-neighbors">Comparison of Linear Regression with K-Nearest Neighbors</h3>
<h2 id="classification">4 - Classification</h2>
<h3 id="an-overview-of-classification">An Overview of Classification</h3>
<h3 id="why-not-linear-regression">Why Not Linear Regression?</h3>
<h3 id="logistic-regression">Logistic Regression</h3>
<h3 id="linear-discriminant-analysis">Linear Discriminant Analysis</h3>
<h3 id="a-comparison-of-classification-methods">A Comparison of Classification Methods</h3>
<h2 id="resampling-methods">5 - Resampling Methods</h2>
<h3 id="cross-validation">Cross-Validation</h3>
<h3 id="the-bootstrap">The Bootstrap</h3>
<h2 id="linear-model-selection-and-regularization">6 - Linear Model Selection and Regularization</h2>
<h3 id="subset-selection">Subset Selection</h3>
<h3 id="shrinkage-methods">Shrinkage Methods</h3>
<h3 id="dimension-reduction-methods">Dimension Reduction Methods</h3>
<h3 id="considerations-in-high-dimensions">Considerations in High Dimensions</h3>
<h2 id="moving-beyond-linearity">7 - Moving Beyond Linearity</h2>
<h3 id="polynomial-regression">Polynomial Regression</h3>
<h3 id="step-functions">Step Functions</h3>
<h3 id="basis-functions">Basis Functions</h3>
<h3 id="regression-splines">Regression Splines</h3>
<h3 id="smoothing-splines">Smoothing Splines</h3>
<h3 id="local-regression">Local Regression</h3>
<h3 id="generalized-additive-models">Generalized Additive Models</h3>
<h2 id="tree-based-methods">8 - Tree-Based Methods</h2>
<h3 id="the-basics-of-decision-trees">The Basics of Decision Trees</h3>
<h3 id="bagging-random-forests-boosting">Bagging, Random Forests, Boosting</h3>
<h2 id="support-vector-machines">9 - Support Vector Machines</h2>
<h3 id="maximal-margin-classifier">Maximal Margin Classifier</h3>
<h3 id="support-vector-classifiers">Support Vector Classifiers</h3>
<h3 id="support-vector-machines-1">Support Vector Machines</h3>
<h3 id="svms-with-more-than-two-classes">SVMs with More than Two Classes</h3>
<h3 id="relationship-to-logistic-regression">Relationship to Logistic Regression</h3>
<h2 id="unsupervised-learning">10 - Unsupervised Learning</h2>
<h3 id="the-challenge-of-unsupervised-learning">The Challenge of Unsupervised Learning</h3>
<h3 id="principal-components-analysis">Principal Components Analysis</h3>
<h3 id="clustering-methods">Clustering Methods</h3>
</body>
</html>
